{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torchtext=0.6，否则导入Field会报错(这个教程似乎比较老啦)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# 导入经典文本数据集工具包\n",
    "import torchtext\n",
    "# 导入英文分词工具包\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "# 导入已经构建完成的Transformer工具包\n",
    "from pyitcast.transformer import TransformerModel\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置Torch下载目录，Torch下载的数据集或模型等都下载到这文件夹\n",
    "os.environ['TORCH_HOME'] = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.data.field.Field object at 0x000001D9D790D590>\n"
     ]
    }
   ],
   "source": [
    "# 将数据进行语料库封装\n",
    "TEXT = torchtext.data.Field(tokenize=get_tokenizer('basic_english'),\n",
    "                            init_token='<sos>',\n",
    "                            eos_token='<eos>',\n",
    "                            lower=True)\n",
    "print(TEXT)   # 返回迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<eos>', '=', 'robert', '<unk>', '=', '<eos>', '<eos>', 'robert', '<unk>', 'is', 'an', 'english', 'film', ',', 'television', 'and', 'theatre', 'actor', '.', 'he', 'had', 'a', 'guest', '@-@', 'starring', 'role', 'on', 'the', 'television', 'series', 'the', 'bill', 'in', '2000', '.', 'this', 'was', 'followed', 'by', 'a', 'starring', 'role', 'in', 'the', 'play', 'herons', 'written', 'by', 'simon', 'stephens', ',', 'which', 'was', 'performed', 'in', '2001', 'at', 'the', 'royal', 'court', 'theatre', '.', 'he', 'had', 'a', 'guest', 'role', 'in', 'the', 'television', 'series', 'judge', 'john', '<unk>', 'in', '2002', '.', 'in', '2004', '<unk>', 'landed', 'a', 'role', 'as', 'craig', 'in', 'the', 'episode', 'teddy', \"'\", 's', 'story', 'of', 'the', 'television', 'series', 'the', 'long', 'firm', 'he']\n"
     ]
    }
   ],
   "source": [
    "# 使用torchtext的数据集方法导入WikiText2数据集\n",
    "train_text, val_text, test_text = torchtext.datasets.WikiText2.splits(text_field=TEXT, root='data')\n",
    "print(test_text.examples[0].text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将训练集文本数据构建一个vocab对象，可以使用vocab对象的stoi方法共包含的不重复的词汇总数\n",
    "TEXT.build_vocab(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建批次数据的函数\n",
    "def batchify(data, batch_size):\n",
    "    \"\"\"\n",
    "    :param data: 之前得到的文本数据 train_text val_text test_text; 使用data.examples[0].text[:10]查看字符\n",
    "    :param batch_size:\n",
    "    :return: [nbatch, batch_size]\n",
    "    \"\"\"\n",
    "    # 第一步使用TEXT的numericalize()将单词映射成连续数字\n",
    "    data = TEXT.numericalize([data.examples[0].text])   # data.examples.__len__()为1 这句是将data的所有单词映射为数字\n",
    "    # 第二步取得需要经过多少次的batch_size后能够遍历完所有数据\n",
    "    nbatch = data.size(0) // batch_size     # data.size() 训练集torch.Size([2086708, 1])\n",
    "\n",
    "    # 利用narrow()对数据进行切割\n",
    "    # 第1个参数 代表横轴切割还是纵轴切割  0-横轴 1-纵轴\n",
    "    # 第2个参数 第3个参数分别代表切割的起始位置和终止位置\n",
    "    data = data.narrow(0, 0, nbatch * batch_size)  # 使用的数据刚好是 整数 个batch_size\n",
    "\n",
    "    # 对data形状进行转变\n",
    "    data = data.view(batch_size, -1).t().contiguous()  # t()让batch_size到列上 [nbatch, batch_size] 即每行就是一个batch\n",
    "    return data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[2, 3],\n",
      "        [5, 6],\n",
      "        [8, 9]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(x.narrow(0, 0, 2))    # 行 左闭右开\n",
    "print(x.narrow(1, 1, 2))    # 列 闭区间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先设置训练数据批次大小\n",
    "batch_size = 20\n",
    "# 设置验证数据和测试数据批次大小\n",
    "eval_batch_size = 10\n",
    "\n",
    "# 获得训练数据、验证数据、测试数据\n",
    "train_data = batchify(train_text, batch_size)   # 将输入返回为[nbatch, batch_size]形状\n",
    "val_data = batchify(val_text, eval_batch_size)\n",
    "test_data = batchify(test_text, eval_batch_size)\n",
    "\n",
    "# 设定句子最大长度\n",
    "bptt = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, i):\n",
    "    \"\"\"\n",
    "    获取batch行文本\n",
    "    :param source:  train_data等  [nbatch, batch_size]\n",
    "    :param i: 批次数\n",
    "    :return: data [bptt, batch_size], target [bapp * batch_size]\n",
    "    \"\"\"\n",
    "    # 确定句子长度值\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "\n",
    "    # 首先得到源数据 取seq_len[通常是bptt]行\n",
    "    data = source[i:i + seq_len]\n",
    "    # 然后得到目标数据\n",
    "    target = source[i + 1:i + 1 + seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 10])\n",
      "torch.Size([350])\n"
     ]
    }
   ],
   "source": [
    "source = test_data      # [nbatch, eval_batch_size]\n",
    "i = 1\n",
    "x, y = get_batch(source, i)\n",
    "print(x.shape, y.shape, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置模型超参数\n",
    "# 通过TEXT.vocab.stoi方法获取不重复的词汇总数\n",
    "ntokens = len(TEXT.vocab.stoi)      # 28785  TEXT相当于是语料库\n",
    "\n",
    "# 设置词嵌入维度的值等于200\n",
    "emsize = 200\n",
    "\n",
    "# 设置前馈全连接层节点数\n",
    "nhid = 200\n",
    "\n",
    "# 设置编码器层的层数\n",
    "nlayers = 2\n",
    "\n",
    "# 多头注意力机制头数\n",
    "nhead = 2\n",
    "\n",
    "# 设置置零比率\n",
    "dropout = .2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将参数传入TransformerModel实例化模型\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
    "\n",
    "# loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# lr\n",
    "lr = 5.\n",
    "\n",
    "# optim\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# 定义lr调整器，使用torch自带的lr_scheduler，将优化器传入\n",
    "schedualer = torch.optim.lr_scheduler.StepLR(optimizer, 1., gamma=.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建训练函数\n",
    "def train():\n",
    "    # 首先开启train模式\n",
    "    model.train()\n",
    "    # 定义初始loss值\n",
    "    total_loss = 0\n",
    "    # 设置打印间隔\n",
    "    log_interval = 200\n",
    "    # 获取当前开始时间\n",
    "    start_time = time.time()\n",
    "    # 遍历训练数据，训练模型\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):  # train_data.size() [104335, 20]\n",
    "        # 通过前面的get_batch获取源数据和目标数据\n",
    "        data, targets = get_batch(train_data, i)    # data [bptt, batch] target形状[bptt * batch]\n",
    "        # 梯度归零\n",
    "        optimizer.zero_grad()\n",
    "        # 通过模型预测输出  维度ntokens即对每个词预测的概率\n",
    "        output = model(data)    # torch.Size([bbpt35, batch20, ntokens28785])\n",
    "        # 计算loss    报错\n",
    "        loss = criterion(output.view(-1, ntokens), targets)     # output.view()后 [bbpt * batch, ntokens] target [bbpt * batch]\n",
    "        # 反传\n",
    "        loss.backward()\n",
    "        # 剪裁梯度，防止梯度爆炸、消失\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), .5)\n",
    "        # 参数更新\n",
    "        optimizer.step()\n",
    "        # 累加损失值\n",
    "        total_loss += loss.item()\n",
    "        # 打印日志\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            # 首先计算平均损失\n",
    "            cur_loss = total_loss / log_interval\n",
    "            # 计算训练到目前的耗时\n",
    "            elapased = time.time() - start_time\n",
    "            # 打印\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(epoch, batch, len(train_data) // bptt, schedualer.get_lr()[0],\n",
    "                                                      elapased * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            # 每个打印批次结束后，将总损失清零\n",
    "            total_loss = 0\n",
    "            # 重新获取下一个打印轮次的开始时间\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建评估函数\n",
    "def evaluate(eval_model, data_source):\n",
    "    \"\"\"\n",
    "    :param eval_model: 训练后的模型\n",
    "    :param data_source: 验证集、测试集数据\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    eval_model.eval()\n",
    "\n",
    "    # 初始化总损失\n",
    "    total_loss = 0\n",
    "    # 模型开启评估模式后，不进行反传，以加快计算\n",
    "    with torch.no_grad():\n",
    "        # 遍历验证数据\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            # 首先通过get_batch()获取源数据和目标数据\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            # 将源数据放入评估模型中，进行预测\n",
    "            output = eval_model(data)\n",
    "            # 对输出张量进行变形，遍历全部词汇的概率分布\n",
    "            output_flat = output.view(-1, ntokens)      # [拉平，总共有多少单词] 每一个单词都有一个概率\n",
    "            # 累加损失\n",
    "            total_loss += criterion(output_flat, targets).item()\n",
    "    # 返回评估的总损失值\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\miniconda3\\envs\\pt20\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:389: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2981 batches | lr 5.00 | ms/batch 486.20 | loss  7.94 | ppl  2814.28\n",
      "| epoch   1 |   400/ 2981 batches | lr 5.00 | ms/batch 392.82 | loss  6.76 | ppl   861.73\n",
      "| epoch   1 |   600/ 2981 batches | lr 5.00 | ms/batch 417.32 | loss  6.35 | ppl   570.85\n",
      "| epoch   1 |   800/ 2981 batches | lr 5.00 | ms/batch 408.30 | loss  6.22 | ppl   504.47\n",
      "| epoch   1 |  1000/ 2981 batches | lr 5.00 | ms/batch 390.57 | loss  6.12 | ppl   453.06\n",
      "| epoch   1 |  1200/ 2981 batches | lr 5.00 | ms/batch 409.86 | loss  6.08 | ppl   437.20\n",
      "| epoch   1 |  1400/ 2981 batches | lr 5.00 | ms/batch 610.35 | loss  6.03 | ppl   417.78\n",
      "| epoch   1 |  1600/ 2981 batches | lr 5.00 | ms/batch 516.26 | loss  6.05 | ppl   422.36\n",
      "| epoch   1 |  1800/ 2981 batches | lr 5.00 | ms/batch 500.04 | loss  5.95 | ppl   382.56\n",
      "| epoch   1 |  2000/ 2981 batches | lr 5.00 | ms/batch 470.24 | loss  5.95 | ppl   382.69\n",
      "| epoch   1 |  2200/ 2981 batches | lr 5.00 | ms/batch 422.73 | loss  5.84 | ppl   342.95\n",
      "| epoch   1 |  2400/ 2981 batches | lr 5.00 | ms/batch 398.05 | loss  5.88 | ppl   358.35\n",
      "| epoch   1 |  2600/ 2981 batches | lr 5.00 | ms/batch 395.02 | loss  5.88 | ppl   358.96\n",
      "| epoch   1 |  2800/ 2981 batches | lr 5.00 | ms/batch 395.58 | loss  5.79 | ppl   328.18\n",
      "------------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 1365.54s | valid loss 3546.22 | \n",
      "------------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2981 batches | lr 4.51 | ms/batch 399.40 | loss  5.78 | ppl   324.98\n",
      "| epoch   2 |   400/ 2981 batches | lr 4.51 | ms/batch 398.59 | loss  5.76 | ppl   316.08\n",
      "| epoch   2 |   600/ 2981 batches | lr 4.51 | ms/batch 399.16 | loss  5.59 | ppl   266.97\n",
      "| epoch   2 |   800/ 2981 batches | lr 4.51 | ms/batch 400.22 | loss  5.63 | ppl   278.18\n",
      "| epoch   2 |  1000/ 2981 batches | lr 4.51 | ms/batch 416.78 | loss  5.58 | ppl   264.92\n",
      "| epoch   2 |  1200/ 2981 batches | lr 4.51 | ms/batch 419.21 | loss  5.61 | ppl   272.04\n",
      "| epoch   2 |  1400/ 2981 batches | lr 4.51 | ms/batch 437.30 | loss  5.62 | ppl   275.99\n",
      "| epoch   2 |  1600/ 2981 batches | lr 4.51 | ms/batch 478.18 | loss  5.65 | ppl   284.89\n",
      "| epoch   2 |  1800/ 2981 batches | lr 4.51 | ms/batch 550.42 | loss  5.58 | ppl   264.68\n",
      "| epoch   2 |  2000/ 2981 batches | lr 4.51 | ms/batch 657.73 | loss  5.61 | ppl   272.29\n",
      "| epoch   2 |  2200/ 2981 batches | lr 4.51 | ms/batch 461.68 | loss  5.50 | ppl   244.56\n",
      "| epoch   2 |  2400/ 2981 batches | lr 4.51 | ms/batch 417.42 | loss  5.57 | ppl   263.28\n",
      "| epoch   2 |  2600/ 2981 batches | lr 4.51 | ms/batch 414.16 | loss  5.59 | ppl   267.26\n",
      "| epoch   2 |  2800/ 2981 batches | lr 4.51 | ms/batch 400.09 | loss  5.51 | ppl   246.84\n",
      "------------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 1375.81s | valid loss 3493.97 | \n",
      "------------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2981 batches | lr 4.29 | ms/batch 415.66 | loss  5.55 | ppl   256.01\n",
      "| epoch   3 |   400/ 2981 batches | lr 4.29 | ms/batch 398.52 | loss  5.54 | ppl   255.13\n",
      "| epoch   3 |   600/ 2981 batches | lr 4.29 | ms/batch 437.76 | loss  5.36 | ppl   213.16\n",
      "| epoch   3 |   800/ 2981 batches | lr 4.29 | ms/batch 444.58 | loss  5.41 | ppl   223.12\n",
      "| epoch   3 |  1000/ 2981 batches | lr 4.29 | ms/batch 538.79 | loss  5.37 | ppl   214.96\n",
      "| epoch   3 |  1200/ 2981 batches | lr 4.29 | ms/batch 615.33 | loss  5.41 | ppl   222.88\n",
      "| epoch   3 |  1400/ 2981 batches | lr 4.29 | ms/batch 559.37 | loss  5.43 | ppl   227.81\n",
      "| epoch   3 |  1600/ 2981 batches | lr 4.29 | ms/batch 434.98 | loss  5.47 | ppl   238.38\n",
      "| epoch   3 |  1800/ 2981 batches | lr 4.29 | ms/batch 432.87 | loss  5.40 | ppl   220.78\n",
      "| epoch   3 |  2000/ 2981 batches | lr 4.29 | ms/batch 444.71 | loss  5.43 | ppl   227.96\n",
      "| epoch   3 |  2200/ 2981 batches | lr 4.29 | ms/batch 456.85 | loss  5.32 | ppl   203.71\n",
      "| epoch   3 |  2400/ 2981 batches | lr 4.29 | ms/batch 485.45 | loss  5.39 | ppl   219.51\n",
      "| epoch   3 |  2600/ 2981 batches | lr 4.29 | ms/batch 560.18 | loss  5.41 | ppl   224.21\n",
      "| epoch   3 |  2800/ 2981 batches | lr 4.29 | ms/batch 571.19 | loss  5.34 | ppl   208.08\n",
      "------------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 1526.10s | valid loss 3453.01 | \n",
      "------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 首先初始化最佳模型损失值\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "# 定义最佳模型，初始化为空\n",
    "best_model = None\n",
    "\n",
    "# 训练\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # 获取当前轮次开始时间\n",
    "    start_time = time.time()\n",
    "    # 直接调用训练函数进行模型训练\n",
    "    train()\n",
    "    # 调用评估函数得到验证集损失\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    # 打印log\n",
    "    print('-' * 90)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '.format(epoch, (time.time() - start_time),\n",
    "                                                                                 val_loss))\n",
    "    print('-' * 90)\n",
    "    # 通过比较当前epoch的损失，获取最佳模型\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "    # 每个epoch后调整优化器学习率\n",
    "    schedualer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------\n",
      "| End of traning | test loss 3832.80\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 添加测试流程代码\n",
    "test_loss = evaluate(best_model, test_data)\n",
    "print('-' * 90)\n",
    "print('| End of traning | test loss {:5.2f}'.format(test_loss))\n",
    "print('-' * 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
