{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2023.11.19\n",
    "尝试使用PyTorch内置ViT分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "os.environ['TORCH_HOME'] = 'weights'\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import argparse\n",
    "import torchmetrics     # 用于精度指标计算\n",
    "# 用于jupyter实时绘图\n",
    "import pylab as pl\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 定义超参 \"\"\"\n",
    "# parser = argparse.ArgumentParser(description='ViT分类模型 2023.11.19')\n",
    "# parser.add_argument('--batch', default=256, help='')\n",
    "\n",
    "# opt = parser.parse_args()\n",
    "# print(opt)\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH = 128\n",
    "NUM_WORKERS = 4\n",
    "DROP_LAST = True\n",
    "LR = 0.005\n",
    "SAVE_DIR = '/Users/sunchengcheng/Projects/D2L/ViT/weights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 构建数据集 \"\"\"\n",
    "process_data = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize([224, 224]),        # int 图片短边会缩放到int，长边相应缩放，不是我们想要的正方形\n",
    "])\n",
    "# CIFAR10 32x32 colour images in 10 classes\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='../data/', train=True, transform=process_data, download=True)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='../data/', transform=process_data, download=True)\n",
    "train_iter = DataLoader(dataset=train_dataset, batch_size=BATCH, shuffle=True, num_workers=NUM_WORKERS, drop_last=DROP_LAST)\n",
    "test_iter = DataLoader(dataset=test_dataset, batch_size=BATCH, shuffle=False, num_workers=NUM_WORKERS, drop_last=DROP_LAST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['classifier.6.weight', 'classifier.6.bias'], unexpected_keys=[])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" 定义ViT_model \"\"\"\n",
    "# ViT_model = torchvision.models.vit_b_16(weights=torchvision.models.ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "# 测试vgg模型，后期注释\n",
    "ViT_model = torchvision.models.vgg11(num_classes=len(train_dataset.classes))\n",
    "weights_vgg = torch.load(f='weights/hub/checkpoints/vgg11-8a719046.pth')\n",
    "del weights_vgg['classifier.6.weight']\n",
    "del weights_vgg['classifier.6.bias']\n",
    "ViT_model.load_state_dict(weights_vgg, strict=False)\n",
    "\n",
    "# ViT_model.heads = nn.Linear(768, train_dataset.classes.__len__())\n",
    "# \"\"\" 测试ViT输入输出 \"\"\"\n",
    "# with torch.no_grad():\n",
    "#     ViT_model.eval()        # 影响BN、dropout层\n",
    "#     x = torch.rand(size=(10, 3, 224, 224))\n",
    "#     y = ViT_model(x)\n",
    "#     print(y.softmax(dim=-1), torch.argmax(torch.softmax(y, dim=-1), dim=-1), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 定义loss、optim \"\"\"\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(params=ViT_model.parameters(), lr=LR)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=train_iter.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_iter, device):\n",
    "    metric_f1 = torchmetrics.F1Score(task='multiclass', num_classes=len(test_iter.dataset.classes)).to(device)\n",
    "    \"\"\" 使用验证集评估模型 \"\"\"\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, (x, y) in enumerate(test_iter):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_hat = model(x)\n",
    "            correct += torch.sum(y_hat.softmax(dim=-1).argmax(dim=-1) == y)\n",
    "            total += x.shape[0]\n",
    "            acc = correct / total\n",
    "            batch_f1 = metric_f1(y_hat.softmax(dim=-1).argmax(dim=-1), y)\n",
    "            print(f'---------->evaluate {i}/{len(test_iter)} acc:{acc.item() * 100:.7f}% batch_f1 {batch_f1 * 100:.4f}% ')\n",
    "        eval_f1 = metric_f1.compute()\n",
    "        print(f'---------->evaluate eval_f1:{eval_f1 * 100:.4f}%')\n",
    "        return acc.item(), eval_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0/100 iter 26/390 lr 0.9890738               acc 23.9583% batch_acc 42.1875% batch_f1 42.1875 \n"
     ]
    }
   ],
   "source": [
    "\"\"\" 训练 \"\"\"\n",
    "max_f1 = 0      # 记录最大f1分数，保存模型\n",
    "lr_iter = []\n",
    "acc_iter = []\n",
    "batch_acc_iter = []\n",
    "batch_f1_iter = []\n",
    "correct = 0\n",
    "total = 0\n",
    "iter = 0\n",
    "device = torch.device('mps')\n",
    "# 精度指标计算\n",
    "metric_acc = torchmetrics.Accuracy(task='multiclass', num_classes=len(train_dataset.classes)).to(device)\n",
    "metric_f1 = torchmetrics.F1Score(task='multiclass', num_classes=len(train_dataset.classes)).to(device)\n",
    "metric_precision = torchmetrics.Precision\n",
    "ViT_model.to(device=device)\n",
    "for epoch in range(EPOCHS):\n",
    "    ViT_model.train()               # 切换到train模式\n",
    "    for i, (x, y) in enumerate(train_iter):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = ViT_model(x)\n",
    "        # 统计精度\n",
    "        correct += torch.sum(y_hat.softmax(dim=-1).argmax(dim=-1) == y)\n",
    "        total += x.shape[0]\n",
    "        acc = correct / total\n",
    "        acc_iter.append(acc.item())\n",
    "        batch_acc = metric_acc(y_hat.softmax(dim=-1).argmax(dim=-1), y)     # 计算batch的精度；已验证metric_acc.compute()与自己写的全局acc同\n",
    "        batch_f1 = metric_f1(y_hat.softmax(dim=-1).argmax(dim=-1), y)\n",
    "        lr_iter.append(scheduler.get_last_lr()[0] * 200)        # 收集学习率放大200倍，方便显示\n",
    "        batch_acc_iter.append(batch_acc.item())\n",
    "        batch_f1_iter.append(batch_f1.item())\n",
    "        iter += 1\n",
    "        print(f'epoch {epoch}/{EPOCHS} iter {i}/{len(train_iter)} lr {lr_iter[-1]:.7f} \\\n",
    "              acc {acc * 100:.4f}% batch_acc {batch_acc * 100:.4f}% batch_f1 {batch_f1 * 100:.4f} ')\n",
    "        # backward update\n",
    "        loss = loss_func(y_hat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # draw acc\n",
    "        pl.clf()\n",
    "        pl.plot(acc_iter, label='acc')\n",
    "        pl.plot(lr_iter, label='lr * 200')\n",
    "        pl.plot(batch_acc_iter, label='batch_acc')\n",
    "        pl.plot(batch_f1_iter, label='batch_f1')\n",
    "        pl.legend(loc='upper right')                # 必须设置，否则pl.plot()的label参数显示不出来\n",
    "        pl.xlabel(f'iters')\n",
    "        display.display(pl.gcf())\n",
    "        display.clear_output(True)\n",
    "    # evalute model\n",
    "    epoch_acc = metric_acc.compute()\n",
    "    epoch_f1 = metric_f1.compute()\n",
    "    metric_acc.reset()\n",
    "    metric_f1.reset()\n",
    "    print(f'epoch {epoch}/{EPOCHS} epoch performance: epoch_acc {epoch_acc * 100:.4f} epoch_f1 {epoch_f1 * 100:.4f}')\n",
    "    acc, eval_f1 = evaluate(ViT_model, test_iter, device)\n",
    "    if eval_f1 > max_f1:\n",
    "        max_f1 = eval_f1\n",
    "        print(f'*************** Find Better Model, Saving Model to {SAVE_DIR} *****************')\n",
    "        torch.save(ViT_model.state_dict(), os.path.join(SAVE_DIR, f'best.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sunchengcheng/miniconda3/envs/pt20/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/Users/sunchengcheng/miniconda3/envs/pt20/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/Users/sunchengcheng/miniconda3/envs/pt20/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/Users/sunchengcheng/miniconda3/envs/pt20/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------->evaluate acc:4.6875000% \n",
      "---------->evaluate acc:5.4687500% \n",
      "---------->evaluate acc:6.2500000% \n",
      "---------->evaluate acc:6.6406250% \n",
      "---------->evaluate acc:7.1874999% \n",
      "---------->evaluate acc:8.3333336% \n",
      "---------->evaluate acc:9.1517858% \n",
      "---------->evaluate acc:9.7656250% \n",
      "---------->evaluate acc:9.3750000% \n",
      "---------->evaluate acc:10.0000001% \n",
      "---------->evaluate acc:10.2272727% \n",
      "---------->evaluate acc:10.5468750% \n",
      "---------->evaluate acc:10.3365384% \n",
      "---------->evaluate acc:9.9330358% \n",
      "---------->evaluate acc:10.0000001% \n",
      "---------->evaluate acc:9.8632812% \n",
      "---------->evaluate acc:9.5588237% \n",
      "---------->evaluate acc:9.6354164% \n",
      "---------->evaluate acc:9.8684214% \n",
      "---------->evaluate acc:10.0781247% \n",
      "---------->evaluate acc:10.1190478% \n",
      "---------->evaluate acc:9.9431820% \n",
      "---------->evaluate acc:10.0543477% \n",
      "---------->evaluate acc:10.0911461% \n",
      "---------->evaluate acc:10.0625001% \n",
      "---------->evaluate acc:9.9759616% \n",
      "---------->evaluate acc:10.1273149% \n",
      "---------->evaluate acc:9.9888392% \n",
      "---------->evaluate acc:9.8599136% \n",
      "---------->evaluate acc:9.8437503% \n",
      "---------->evaluate acc:9.8790325% \n",
      "---------->evaluate acc:9.9121094% \n",
      "---------->evaluate acc:9.7537875% \n",
      "---------->evaluate acc:9.6966915% \n",
      "---------->evaluate acc:9.5535718% \n",
      "---------->evaluate acc:9.5052086% \n",
      "---------->evaluate acc:9.5861487% \n",
      "---------->evaluate acc:9.7039476% \n",
      "---------->evaluate acc:9.7756408% \n",
      "---------->evaluate acc:9.6484378% \n",
      "---------->evaluate acc:9.6036583% \n",
      "---------->evaluate acc:9.7098216% \n",
      "---------->evaluate acc:9.6656978% \n",
      "---------->evaluate acc:9.5880680% \n",
      "---------->evaluate acc:9.5833331% \n",
      "---------->evaluate acc:9.5108695% \n",
      "---------->evaluate acc:9.5744684% \n",
      "---------->evaluate acc:9.6354164% \n",
      "---------->evaluate acc:9.6619897% \n",
      "---------->evaluate acc:9.6562497% \n",
      "---------->evaluate acc:9.6507356% \n",
      "---------->evaluate acc:9.6153848% \n",
      "---------->evaluate acc:9.8466977% \n",
      "---------->evaluate acc:9.7511575% \n",
      "---------->evaluate acc:9.6590906% \n",
      "---------->evaluate acc:9.6819200% \n",
      "---------->evaluate acc:9.6765354% \n",
      "---------->evaluate acc:9.6713364% \n",
      "---------->evaluate acc:9.6398301% \n",
      "---------->evaluate acc:9.5572919% \n",
      "---------->evaluate acc:9.6055329% \n",
      "---------->evaluate acc:9.6522175% \n",
      "---------->evaluate acc:9.6478172% \n",
      "---------->evaluate acc:9.6679688% \n",
      "---------->evaluate acc:9.6153848% \n",
      "---------->evaluate acc:9.6354164% \n",
      "---------->evaluate acc:9.7248137% \n",
      "---------->evaluate acc:9.7426474% \n",
      "---------->evaluate acc:9.7373188% \n",
      "---------->evaluate acc:9.7098216% \n",
      "---------->evaluate acc:9.7711265% \n"
     ]
    }
   ],
   "source": [
    "ViT_model.to(torch.device('mps'))\n",
    "evaluate(ViT_model, test_iter, torch.device('mps'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 4096])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU(inplace=True)\n",
       "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ViT_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
