{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n    深度学习框架会在后端自动构建计算图，利用计算图，系统可以了解所有依赖关系，\\n并且选择性地并行执行多个不相互依赖的任务提高速度 \\n    通常情况下，单个操作符将使用所有CPU或单个GPU上所有计算资源。如，即时在一台\\n机器上有多个CPU处理器，dot操作符也将使用所有核心(和线程)。这样的行为同样适用于\\n单个GPU。因此并行化对单设备计算机不是很有用，而常用于多GPU之间\\n    预热设备确保缓存的作用不影响最终的结果--对设备执行一次传递\\n    torch.cuda.synchronize()会等待一个CUDA设备的所有流中的所有核心的计算完成，\\n函数接受一个device参数，代表哪个设备需要同步。如果device=None，将使用current_device()找出当前设备\\n    如果删除两任务之间的synchronize语句，系统就可以在两个设备上自动实现并行计算。\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "    深度学习框架会在后端自动构建计算图，利用计算图，系统可以了解所有依赖关系，\n",
    "并且选择性地并行执行多个不相互依赖的任务提高速度 \n",
    "    通常情况下，单个操作符将使用所有CPU或单个GPU上所有计算资源。如，即时在一台\n",
    "机器上有多个CPU处理器，dot操作符也将使用所有核心(和线程)。这样的行为同样适用于\n",
    "单个GPU。因此并行化对单设备计算机不是很有用，而常用于多GPU之间\n",
    "    预热设备确保缓存的作用不影响最终的结果--对设备执行一次传递\n",
    "    torch.cuda.synchronize()会等待一个CUDA设备的所有流中的所有核心的计算完成，\n",
    "函数接受一个device参数，代表哪个设备需要同步。如果device=None，将使用current_device()找出当前设备\n",
    "    如果删除两任务之间的synchronize语句，系统就可以在两个设备上自动实现并行计算。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from d2l import torch as d2l\n",
    "\n",
    "devices = d2l.try_all_gpus()\n",
    "\n",
    "def run(x):\n",
    "    return [x.mm(x) for _ in range(50)]\n",
    "\n",
    "x_gpu1 = torch.randn(size=(400, 400), device=devices[0])\n",
    "x_gpu2 = torch.randn(size=(400, 400), device=devices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(x_gpu1)\n",
    "torch.cuda.synchronize(device=devices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU0 time: 0.0140 sec\n",
      "GPU0 time: 0.0130 sec\n"
     ]
    }
   ],
   "source": [
    "with d2l.Benchmark('GPU0 time'):\n",
    "    run(x_gpu1)\n",
    "    torch.cuda.synchronize(device=devices[0])\n",
    "with d2l.Benchmark('GPU0 time'):\n",
    "    run(x_gpu2)\n",
    "    torch.cuda.synchronize(device=devices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU0 time: 0.0260 sec\n"
     ]
    }
   ],
   "source": [
    "with d2l.Benchmark('GPU0 time'):\n",
    "    run(x_gpu1)\n",
    "    run(x_gpu2)\n",
    "    torch.cuda.synchronize(device=devices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在gpu0上运行: 0.0140 sec\n",
      "复制到CPU: 0.0270 sec\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 并行计算与通信 \"\"\"\n",
    "\"\"\" 在不同设备之间移动数据，CPU&GPU GPU与GPU \"\"\"\n",
    "\"\"\"\n",
    "Dataloader()加载数据时参数pin_memory=True为生成的Tensor放在锁页\n",
    "Tensor.to or .cuda()参数non_blocking=True意为只放入GPU而不取出\n",
    "\"\"\"\n",
    "def copy_to_cpu(x, non_blocking=False):\n",
    "    return [y.to('cpu', non_blocking=non_blocking) for y in x]\n",
    "\n",
    "with d2l.Benchmark('在gpu0上运行'):\n",
    "    y = run(x_gpu1)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "with d2l.Benchmark('复制到CPU'):\n",
    "    y_cpu = copy_to_cpu(y)\n",
    "    torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在GPU0上运行并复制到CPU: 0.0170 sec\n"
     ]
    }
   ],
   "source": [
    "with d2l.Benchmark('在GPU0上运行并复制到CPU'):\n",
    "    y = run(x_gpu1)\n",
    "    copy_to_cpu(y, non_blocking=True)\n",
    "    torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('d2l')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6330671cf013f9362c2cd1e55198b197db19b836250c5c20ff0742b9c9511c0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
